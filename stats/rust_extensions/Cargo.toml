[package]
name = "gemma-extensions"
version.workspace = true
edition.workspace = true
authors.workspace = true
description = "High-performance Rust extensions for Gemma chatbot operations"
license.workspace = true
homepage = "https://github.com/gemma-chatbot/extensions"
repository = "https://github.com/gemma-chatbot/extensions"
keywords = ["nlp", "tokenization", "performance", "gemma", "chatbot"]
categories = ["text-processing", "science"]

[lib]
name = "gemma_extensions"
crate-type = ["cdylib"]

[dependencies]
pyo3 = { workspace = true, features = ["extension-module", "abi3-py311"] }
tokio.workspace = true
# TODO: Re-enable when pyo3-asyncio is updated for PyO3 0.24+
# pyo3-asyncio = { workspace = true, features = ["tokio-runtime"] }
rayon = { version = "1.10", optional = true }
memchr = "2.7"
bytemuck = { version = "1.16", features = ["derive"] }
ndarray = "0.16"
lru = "0.12"
parking_lot = "0.12"
dashmap = "6.0"
fnv = "1.0"
ahash = "0.8"
smallvec = "1.13"
once_cell = "1.19"
tracing = "0.1"
anyhow = "1.0"
thiserror = "1.0"
serde.workspace = true
serde_json = "1.0"
regex = "1.10"
tracing-subscriber = "0.3"

# SIMD and performance optimizations
wide = "0.7"
num-traits = "0.2"
num-complex = "0.4"
approx = "0.5"

# Optional features for specific backends
hf-hub = { version = "0.3", optional = true }
candle-core = { workspace = true, optional = true }
candle-nn = { workspace = true, optional = true }

# Redis and RAG system dependencies
redis.workspace = true
bb8-redis = "0.24"
bb8 = "0.9"

# Vector similarity and HNSW
hnsw = "0.11"
linfa = "0.7"
linfa-clustering = "0.7"
faiss = { version = "0.12", optional = true }

# HTTP client for external API integrations
reqwest = { version = "0.12", features = ["json", "stream"] }
url = "2.5"
uuid.workspace = true

# Document processing and text utilities
tiktoken-rs = "0.5"
unicode-segmentation = "1.10"
pulldown-cmark = "0.9"
scraper = "0.18"
pdf-extract = { version = "0.7", optional = true }
csv = "1.3"
zip = "2.2"
docx-rs = { version = "0.4.18", optional = true }

# Embedding and ML utilities
candle-transformers = { workspace = true, optional = true }
tokenizers = { version = "0.15", optional = true }
ort = { version = "2.0.0-rc.10", optional = true }

# Async utilities
futures = "0.3"
stream-cancel = "0.8"
async-stream = "0.3"

# Compression for document storage
flate2 = "1.0"
zstd = "0.13"

# Configuration and logging
config = "0.14"
log = "0.4"
libc = "0.2.175"
tempfile = "3.8"
lazy_static = "1.4"
num_cpus = "1.16"

[target.'cfg(target_arch = "x86_64")'.dependencies]
# x86_64 specific SIMD optimizations
target-lexicon = "0.12"

[target.'cfg(target_arch = "aarch64")'.dependencies]
# ARM64 specific optimizations
target-lexicon = "0.12"

[dev-dependencies]
criterion = { version = "0.5", features = ["html_reports"] }
proptest = "1.4"
tokio-test = "0.4"
tempfile = "3.8"

[features]
default = ["simd", "parallel", "redis-backend"]
simd = []
parallel = ["rayon"]
huggingface = ["hf-hub"]
candle = ["candle-core", "candle-nn"]
transformers = ["candle-transformers", "tokenizers"]
faiss-backend = ["faiss"]
onnx-runtime = ["ort"]
pdf-support = ["pdf-extract"]
docx-support = ["docx-rs"]
document-processing = ["pdf-support", "docx-support"]
redis-backend = []
full-rag = ["transformers", "pdf-support", "faiss-backend"]
gemma-cpp = []
debug = []


[[bench]]
name = "tokenizer_bench"
harness = false

[[bench]]
name = "tensor_bench"
harness = false

[[bench]]
name = "cache_bench"
harness = false

[package.metadata.maturin]
python-source = "python"
module-name = "gemma_extensions._gemma_extensions"
